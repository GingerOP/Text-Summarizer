# -*- coding: utf-8 -*-
"""Text_Sumerizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100RATCWVkUHW0FLZCTvkCGyYqR2G6eUj
"""

!pip install --upgrade transformers[sentencepiece] datasets sacrebleu rouge_score py7zr --no-cache-dir

!pip install --upgrade accelerate
!pip uninstall -y transformers accelerate
!pip install transformers accelerate

!pip install datasets
!pip install evaluate

from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset, load_from_disk
import matplotlib.pyplot as plt
import pandas as pd
import evaluate

import nltk
from nltk.tokenize import sent_tokenize
from tqdm import tqdm
import torch
nltk.download("punkt")

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

device

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_ckpt = "google/pegasus-cnn_dailymail"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt, ignore_mismatched_sizes=True)

from datasets import load_dataset

dataset_samsum = load_dataset("knkarthick/samsum")
print(dataset_samsum)

dataset_samsum['train'][0]

dataset_samsum['train']['dialogue'][0]

dataset_samsum['train'][1]['summary']

split_lengths=[len(dataset_samsum[split]) for split in dataset_samsum]
print(split_lengths)

print(dataset_samsum['train'].column_names)

print(dataset_samsum['test'][1]['dialogue'])
print("\nSummary:")
print(dataset_samsum['test'][1]['summary'])

def convert_examples_to_features(example_batch):
    # make sure dialogues and summaries are plain strings
    dialogues = [str(x) for x in example_batch["dialogue"]]
    summaries = [str(x) for x in example_batch["summary"]]

    model_inputs = tokenizer(
        dialogues,
        max_length=1024,
        truncation=True,
        padding="max_length"
    )

    labels = tokenizer(
        text_target=summaries,
        max_length=128,
        truncation=True,
        padding="max_length"
    )

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

dataset_samsum_pt = dataset_samsum.map(
    convert_examples_to_features,
    batched=True
)

dataset_samsum_pt['train']

dataset_samsum_pt['train']['input_ids'][1]

dataset_samsum_pt['train']['attention_mask'][1]

#training
from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

!pip install -U transformers
# then restart your runtime/kernel

from transformers import TrainingArguments, Trainer

trainer_args = TrainingArguments(
    output_dir='pegasus-samsum',
    num_train_epochs=1,
    warmup_steps=500,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=0.01,
    logging_steps=10,
    save_steps=int(1e6),
    gradient_accumulation_steps=16
)

from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=trainer_args,
    tokenizer=tokenizer,
    data_collator=seq2seq_data_collator,
    train_dataset=dataset_samsum_pt["test"],       # usually use "train" here
    eval_dataset=dataset_samsum_pt["validation"]
)

trainer.train()

# Evaluation
def generate_batch_sized_chunks (list_of_elements, batch_size) :
  """split the dataset into smaller batches that we can process simultaneously
  Yield successive batch-sized chunks from list_of_elements."""
  for i in range(0, len(list_of_elements), batch_size):
      yield list_of_elements[i : i + batch_size]

from tqdm import tqdm

def calculate_metric_on_test_ds(
    dataset,
    metric,
    model,
    tokenizer,
    batch_size=16,
    device="cuda",
    column_text="article",
    column_summary="highlights"
):
    # Split dataset into batches
    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)
    ):
        # Tokenize inputs
        inputs = tokenizer(
            article_batch,
            max_length=1024,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        # Move to device and generate summaries
        summaries = model.generate(
            input_ids=inputs["input_ids"].to(device),
            attention_mask=inputs["attention_mask"].to(device),
            length_penalty=0.8,   # discourage overly long sequences
            num_beams=8,
            max_length=128
        )

        # Decode summaries
        decoded_summaries = [
            tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            for s in summaries
        ]

        # Clean up decoded summaries
        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]

        # Add predictions and references to the metric
        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    # Compute final ROUGE score
    score = metric.compute()
    return score

rouge_names = ["rouge1", "rouge2", "rougeL", "RougeLsum"]
rouge_metric = evaluate. load('rouge')

# Calculate metric on test dataset
score = calculate_metric_on_test_ds(
    dataset_samsum["test"][:10],    # first 10 samples
    rouge_metric,
    trainer.model,
    tokenizer,
    batch_size=2,
    column_text="dialogue",
    column_summary="summary"
)

# ROUGE metrics
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

# Extract float values directly
rouge_dict = {rn: score[rn] for rn in rouge_names}

# Convert to DataFrame
import pandas as pd
df = pd.DataFrame(rouge_dict, index=["pegasus"])
print(df)

model.save_pretrained("pegasus-samsum-model")

tokenizer.save_pretrained("tokenizer")

tokenizer = AutoTokenizer.from_pretrained("/content/tokenizer")

gen_kwargs = {"length_penalty": 0.8, "num_beams": 8, "max_length": 128}

sample_text = dataset_samsum["test"] [0] ["dialogue"]
reference = dataset_samsum ["test"] [0] ["summary"]

pipe = pipeline("summarization", model="pegasus-samsum-model", tokenizer=tokenizer)

print("Dialogue:")
print(sample_text)

print("\nReference Summary:")
print(reference)

print("\nModel Summary:")
print(pipe(sample_text, **gen_kwargs)[0]["summary_text"])

